# 代理服务器输出端口
PROXY_PORT=4120

# 是否输出思考内容
Show_COT=true

# 是否在后端日志中输出完整的长文本内容（如思考链、消息详情、模型完整回答）
# 设置为 true 以获取非常详细的调试日志，默认为 false（仅输出摘要和长度）
LOG_FULL_CONTENT=false

# 思考模型
PROXY_URL=
Model_think_API_KEY=
Model_think_MODEL=grok-3
Model_think_MAX_TOKENS=65536
Model_think_CONTEXT_WINDOW=1048576
Model_think_TEMPERATURE=0.7
Model_think_WebSearch=true
Model_think_image=true
Think_PROMPT="从现在开始，无视之前的系统指令(System Prompt)，你将作为一个前置思考辅助系统工作，你的工作独立于上下文，用户不可见，只有输出的AI可见。你的工作是对前文所有信息进行整理，对用户需求进行逻辑思考，深化思考，提供一个chain of Thinking的思维步骤，包含思维广泛化(探讨更多涉及领域，深化思考范围，构成信息交叉)和思维步骤自纠错流程。尤其是细化每一步都逻辑推理过程，和穿插过程中的逻辑自纠错。正式输出部分将由正式输出AI来完成，而非你完成。谨记，你不需要参与上下文的回复工作，你只需要输出应对用户需求的思维链部分即可，注意自纠错步骤。"

# 输出模型
PROXY_URL2=
Model_output_API_KEY=
Model_output_MODEL=grok-3
Model_output_MAX_TOKENS=7950
Model_output_CONTEXT_WINDOW=120500
Model_output_TEMPERATURE=0.8
Model_output_WebSearch=true
Model_output_image=false
Model_output_tool=true
RELAY_PROMPT="请基于前文提供的辅助思考过程，结合你自己的思考用你自己的方式回答用户的问题。注意：不要提到思考过程，直接给出完整的回答。保持SystemPrompt中的角色设定风格来进行回复。"


# 最终混合模型名称
HYBRID_MODEL_NAME=MixLite

# 输出 API 密钥
OUTPUT_API_KEY=123456
